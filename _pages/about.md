---
permalink: /
title: "Homepage"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

> Act as if what you do makes a difference. It does.
\- William James

Welcome üëã! My name is Georgios (but please call me *Yiorgos*) Chochlakis. I am a *Computer Science Ph.D. fellow* at the *University of Southern California* and [Signal Analysis and Interpretation Laboratory](https://sail.usc.edu/), supervised by prof. [Shrikanth Narayanan](https://sail.usc.edu/people/shri.php). I am currently interested and actively publishing on (what I call) **complex subjective tasks**, such as emotion recognition, with LLMs. To define terms, *complex* refers to *inherently difficult, usually multilabel, problems with interrelated labels*, and *subjective* refers to settings where *people care reasonably disagree about their interpretation of a stimulus*. Why are these problems important, you ask? Plenty of reasons actually, you're gonna have to read my papers for that, but let me try to summarize that before you Cmd/Ctrl+W out of here. First, what I would call the resurgence of Aristotelian logic: how do verifiable rewards (and veriable rewards only) influence the behavior of LLMs in domains where there is no right and wrong? I am collecting a multimodal dataset to figure that out at the moment. Second, how are you gonna claim AGI when you test your model on simplified problems? Really, single-label settings? Your model cannot do proper, multi-label classification and you think it's gonna take over the world? The real world is murky and fuzzy, not "single-label".

Previously, before my academic enlightenment, I was publishing SOTA results. I have mostly focused on multilingual emotion recognition from text on social media. I have developed emotion recognition methods that can be deployed in dynamic environments, such as multilingual settings, different desired emotions, and different annotation formats (e.g., transitioning from individual emotions to clusters of emotions). The defining feature of my approach is the ability to control extracted features by creating a label space along the input word embedding space.

Before I came to the US, I studied for a joint M.Eng. and B.Sc. in Electrical ‚ö° & Computer üñ•Ô∏è Engineering at the *National Technical University of Athens* (NTUA), with a major in Computer Science. I did research on Deep Learning algorithms as part of my undergraduate diploma [thesis](http://artemis.cslab.ece.ntua.gr:8080/jspui/handle/123456789/17793) under the supervision of prof. [Alexandros Potamianos](https://slp.cs.ece.ntua.gr/potam/), where I examined generative models in a [Zero-shot Learning](https://en.wikipedia.org/wiki/Zero-shot_learning) setting.

I also pretend to care about the philosophy of science, scientific rigor and reproducibility, having replicated (or failed to do so) results from published work (see [here](https://github.com/gchochla/Deep-Representations-of-Visual-Descriptions) where I corrected an [error](https://github.com/gchochla/Deep-Representations-of-Visual-Descriptions/commit/bb5cfa3d27a7677bbbf16896a1917e9b5227596e) in the paper, [here](https://github.com/gchochla/capsules-utils), [here](https://github.com/gchochla/stackgan-pp), etc.), and I do my best to speedrun through books I should have read as an undergrad.

In my free time, I sleep, work out, and experience existential angst from the ghost of Ancient Greece and what could (should) have been.
